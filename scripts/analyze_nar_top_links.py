#!/usr/bin/env python3
"""NAR ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã®è©³ç´°åˆ†æ - ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒªãƒ³ã‚¯æ¢ç´¢"""

import sys
sys.path.insert(0, '/app')

from nankan_predictor.scraper.base import BaseScraper
from bs4 import BeautifulSoup
import re

def analyze_nar_top_structure():
    """NAR ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰æœˆåˆ¥/ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒªãƒ³ã‚¯æ§‹é€ ã‚’åˆ†æ"""
    
    scraper = BaseScraper(use_cache=False)
    html = scraper.get("https://nar.netkeiba.com/")
    soup = BeautifulSoup(html, 'html.parser')
    
    print("ã€NAR ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸æ§‹é€ åˆ†æã€‘\n")
    
    # ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    links = soup.find_all('a', href=True)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã«ãƒªãƒ³ã‚¯ã‚’åˆ†é¡
    print(f"âœ… ç·ãƒªãƒ³ã‚¯æ•°: {len(links)}\n")
    
    # race_id ãƒªãƒ³ã‚¯
    race_id_links = [a for a in links if 'race_id=' in a.get('href', '')]
    print(f"ğŸ”— race_id ãƒªãƒ³ã‚¯: {len(race_id_links)} å€‹")
    if race_id_links:
        print(f"   ã‚µãƒ³ãƒ—ãƒ«: {race_id_links[0].get('href')}")
    
    # æœˆåˆ¥/ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒªãƒ³ã‚¯ï¼ˆæœˆã®åå‰ã‚’å«ã‚€ã‚‚ã®ï¼‰
    print(f"\nã€æœˆåˆ¥é–¢é€£ãƒªãƒ³ã‚¯æ¢ç´¢ã€‘")
    
    months = ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ', '7æœˆ', '8æœˆ', '9æœˆ', '10æœˆ', '11æœˆ', '12æœˆ']
    month_links = {}
    
    for link in links:
        text = link.get_text(strip=True)
        href = link.get('href', '')
        
        for month in months:
            if month in text:
                if month not in month_links:
                    month_links[month] = []
                month_links[month].append((text, href))
    
    if month_links:
        print(f"âœ… æœˆåˆ¥ãƒªãƒ³ã‚¯æ¤œå‡º: {len(month_links)} æœˆ")
        for month, links_list in sorted(month_links.items()):
            print(f"   {month}: {len(links_list)} å€‹")
            for text, href in links_list[:2]:
                print(f"      - {text[:30]}: {href[:60]}")
    else:
        print("âŒ æœˆåˆ¥ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªãƒ³ã‚¯ï¼ˆYYYYMMDD å½¢å¼ãªã©ï¼‰
    print(f"\nã€æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªãƒ³ã‚¯æ¢ç´¢ã€‘")
    date_pattern = re.compile(r'\d{4}[-/]\d{1,2}[-/]\d{1,2}|\d{8}')
    date_links = []
    
    for link in links:
        href = link.get('href', '')
        text = link.get_text(strip=True)
        
        if date_pattern.search(href) or date_pattern.search(text):
            date_links.append((text, href))
    
    if date_links:
        print(f"âœ… æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªãƒ³ã‚¯: {len(date_links)} å€‹")
        for text, href in date_links[:5]:
            print(f"   {text[:20]}: {href[:60]}")
    else:
        print("âŒ æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    # ã€Œé–‹å‚¬æƒ…å ±ã€ã€Œæˆç¸¾ã€ãªã©æƒ…å ±ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯
    print(f"\nã€æƒ…å ±ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯æ¢ç´¢ã€‘")
    info_keywords = ['é–‹å‚¬', 'æˆç¸¾', 'ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–', 'ãƒãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼', 'éå»', 'ãƒ¬ãƒ¼ã‚¹ä¸€è¦§']
    info_links = {}
    
    for link in links:
        text = link.get_text(strip=True)
        href = link.get('href', '')
        
        for keyword in info_keywords:
            if keyword in text:
                if keyword not in info_links:
                    info_links[keyword] = []
                info_links[keyword].append((text, href))
    
    if info_links:
        print(f"âœ… æƒ…å ±ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯æ¤œå‡º: {len(info_links)} ãƒ‘ã‚¿ãƒ¼ãƒ³")
        for keyword, links_list in info_links.items():
            print(f"   {keyword}: {len(links_list)} å€‹")
            for text, href in links_list[:1]:
                print(f"      - {href[:70]}")
    else:
        print("âŒ æƒ…å ±ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    # é–‹å‚¬äºˆå®šæ—¥ãŠã‚ˆã³ç›´è¿‘ã®é–‹å‚¬æ—¥
    print(f"\nã€é–‹å‚¬äºˆå®š/å®Ÿç¸¾é–¢é€£ã® div/è¦ç´ æ¢ç´¢ã€‘")
    divs = soup.find_all('div', class_=lambda x: x and ('plan' in x.lower() or 'race' in x.lower() or 'schedule' in x.lower()))
    if divs:
        print(f"âœ… é–‹å‚¬é–¢é€£ div: {len(divs)} å€‹")
        for div in divs[:3]:
            print(f"   class={div.get('class')}, text={div.get_text(strip=True)[:50]}")
    
    # nav/aside ãƒªãƒ³ã‚¯æ§‹é€ 
    print(f"\nã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³æ§‹é€ ã€‘")
    navs = soup.find_all(['nav', 'aside'])
    if navs:
        print(f"âœ… nav/aside: {len(navs)} å€‹")
        for nav in navs[:2]:
            nav_links = nav.find_all('a', href=True)
            if nav_links:
                print(f"   å†…éƒ¨ãƒªãƒ³ã‚¯æ•°: {len(nav_links)}")
                print(f"   ã‚µãƒ³ãƒ—ãƒ«: {nav_links[0].get('href')}")

if __name__ == "__main__":
    analyze_nar_top_structure()
