#!/usr/bin/env python3
"""NAR çµæœãƒšãƒ¼ã‚¸ã®æ§‹é€ åˆ†æ"""

import sys
sys.path.insert(0, '/app')

from nankan_predictor.scraper.nar_race_list import NARRaceListScraper
from bs4 import BeautifulSoup

def analyze_result_page():
    """NAR çµæœãƒšãƒ¼ã‚¸ã® HTML æ§‹é€ ã‚’åˆ†æ"""
    
    # ãƒ¬ãƒ¼ã‚¹ID ã‚’å–å¾—
    list_scraper = NARRaceListScraper(use_cache=False)
    race_ids = list_scraper.get_latest_races()
    
    if not race_ids:
        print("âŒ ãƒ¬ãƒ¼ã‚¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    race_id = race_ids[0]
    print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ã‚¹ID: {race_id}")
    
    # çµæœãƒšãƒ¼ã‚¸ã‚’å–å¾—
    from nankan_predictor.scraper.base import BaseScraper
    scraper = BaseScraper(use_cache=False)
    
    result_url = f"https://nar.netkeiba.com/race/result.html?race_id={race_id}"
    print(f"\nğŸ“¥ å–å¾—ä¸­: {result_url}")
    
    try:
        html = scraper.get(result_url)
        soup = BeautifulSoup(html, 'html.parser')
        
        # ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º
        print(f"âœ… HTML ã‚µã‚¤ã‚º: {len(html)} bytes")
        
        print("\nã€ãƒšãƒ¼ã‚¸æ§‹é€ åˆ†æã€‘")
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±
        tables = soup.find_all('table')
        print(f"âœ… ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
        
        # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®è©³ç´°
        for i, table in enumerate(tables):
            rows = table.find_all('tr')
            print(f"\n   ãƒ†ãƒ¼ãƒ–ãƒ« {i}: {len(rows)} è¡Œ")
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤º
            if rows:
                # ãƒ˜ãƒƒãƒ€ãƒ¼
                header = rows[0]
                cols = header.find_all(['th', 'td'])
                col_names = [col.get_text(strip=True) for col in cols[:15]]
                if col_names:
                    print(f"      ãƒ˜ãƒƒãƒ€ãƒ¼: {col_names}")
                
                # æœ€åˆã®ãƒ‡ãƒ¼ã‚¿è¡Œ
                if len(rows) > 1:
                    data = rows[1]
                    data_cols = data.find_all(['td'])
                    data_vals = [col.get_text(strip=True) for col in data_cols[:15]]
                    if data_vals:
                        print(f"      ãƒ‡ãƒ¼ã‚¿1: {data_vals}")
        
        # ãƒ¬ãƒ¼ã‚¹æƒ…å ±ï¼ˆæ—¥æ™‚ã€é¦¬å ´ã€æ¨™æº–ã‚¿ã‚¤ãƒ ãªã©ï¼‰
        print(f"\nâœ… ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±æ¤œç´¢:")
        
        # ã€Œç¬¬ã€ãªã©ã®é€šå¸¸ã®æ–‡å­—åˆ—ã‹ã‚‰æ—¥ä»˜ã‘ã‚’æ¨æ¸¬
        div_list = soup.find_all('div')
        
        for div in div_list:
            text = div.get_text(strip=True)
            if 'ç¬¬' in text and 'æ—¥ç›®' in text:
                print(f"   é–‹å‚¬æƒ…å ±: {text[:100]}")
                break
        
        # æ‰•æˆ»æƒ…å ±
        print(f"\nâœ… æ‰•æˆ»æƒ…å ±æ¤œç´¢:")
        payout_text = soup.find_all(string=lambda text: text and 'é¦¬é€£' in text)
        if payout_text:
            print(f"   é¦¬é€£æƒ…å ±è¦‹ã¤ã‹ã‚Š: {len(payout_text)} ä»¶")
            if payout_text:
                print(f"   ã‚µãƒ³ãƒ—ãƒ«: {payout_text[0][:100]}")
        
        # ãƒªãƒ³ã‚¯æƒ…å ±
        links = soup.find_all('a', href=True)
        horse_links = [a for a in links if '/horse/' in a.get('href', '')]
        print(f"\nâœ… é¦¬æƒ…å ±ãƒªãƒ³ã‚¯: {len(horse_links)}")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    analyze_result_page()
