#!/usr/bin/env python3
"""NAR å‡ºèµ°è¡¨ãƒšãƒ¼ã‚¸ã®æ§‹é€ åˆ†æ"""

import sys
sys.path.insert(0, '/app')

from nankan_predictor.scraper.nar_race_list import NARRaceListScraper
from bs4 import BeautifulSoup
import json

def analyze_shutuba_page():
    """NAR å‡ºèµ°è¡¨ãƒšãƒ¼ã‚¸ã® HTML æ§‹é€ ã‚’åˆ†æ"""
    
    # ã¾ãšãƒ¬ãƒ¼ã‚¹ID ã‚’å–å¾—
    list_scraper = NARRaceListScraper(use_cache=False)
    race_ids = list_scraper.get_latest_races()
    
    if not race_ids:
        print("âŒ ãƒ¬ãƒ¼ã‚¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    race_id = race_ids[0]
    print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ã‚¹ID: {race_id}")
    
    # å‡ºèµ°è¡¨ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    from nankan_predictor.scraper.base import BaseScraper
    scraper = BaseScraper(use_cache=False)
    
    shutuba_url = f"https://nar.netkeiba.com/race/shutuba.html?race_id={race_id}"
    print(f"\nğŸ“¥ å–å¾—ä¸­: {shutuba_url}")
    
    try:
        html = scraper.get(shutuba_url)
        soup = BeautifulSoup(html, 'html.parser')
        
        # ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º
        print(f"âœ… HTML ã‚µã‚¤ã‚º: {len(html)} bytes")
        
        # ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã®ä½ç½®ã‚’æ¢ã™
        print("\nã€ãƒšãƒ¼ã‚¸æ§‹é€ åˆ†æã€‘")
        
        # ãƒ¬ãƒ¼ã‚¹æƒ…å ±ï¼ˆæ—¥æ™‚ã€æ¨™æº–ã‚¿ã‚¤ãƒ ã€é¦¬å ´çŠ¶æ…‹ãªã©ï¼‰
        race_info = soup.find('div', class_='race_info')
        if race_info:
            print("âœ… race_info div ã‚’æ¤œå‡º")
            print(f"   ãƒ†ã‚­ã‚¹ãƒˆ: {race_info.get_text()[:200]}")
        
        # å‡ºèµ°é¦¬ãƒ†ãƒ¼ãƒ–ãƒ«
        tables = soup.find_all('table')
        print(f"\nâœ… ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
        
        # main ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
        for i, table in enumerate(tables):
            rows = table.find_all('tr')
            print(f"   ãƒ†ãƒ¼ãƒ–ãƒ« {i}: {len(rows)} è¡Œ")
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ç¢ºèª
            if rows:
                header = rows[0]
                cols = header.find_all(['th', 'td'])
                col_names = [col.get_text(strip=True) for col in cols[:10]]
                if col_names:
                    print(f"      ãƒ˜ãƒƒãƒ€ãƒ¼: {col_names}")
        
        # div class="RaceData" ãªã©ã®å…¬é–‹æƒ…å ±
        race_data_div = soup.find('div', class_='RaceData')
        if race_data_div:
            print(f"\nâœ… RaceData div ã‚’æ¤œå‡º")
            text = race_data_div.get_text(strip=True)
            print(f"   ãƒ†ã‚­ã‚¹ãƒˆ: {text[:300]}")
        
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¿ã‚°å†…ã® JSON ãªã©
        scripts = soup.find_all('script')
        print(f"\nâœ… ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¿ã‚°æ•°: {len(scripts)}")
        
        # æœ€åˆã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆåˆæœŸåŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ã“ã¨ãŒå¤šã„ï¼‰
        if scripts:
            first_script = scripts[0].string
            if first_script:
                print(f"   æœ€åˆã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæœ€åˆã®300æ–‡å­—ï¼‰:")
                print(f"   {first_script[:300]}")
        
        # ãƒªãƒ³ã‚¯æƒ…å ±ï¼ˆé¦¬è©³ç´°ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ï¼‰
        links = soup.find_all('a', href=True)
        print(f"\nâœ… ãƒªãƒ³ã‚¯æ•°: {len(links)}")
        
        # é¦¬æƒ…å ±ãƒªãƒ³ã‚¯ï¼ˆ/race/horse/ ãªã©ï¼‰
        horse_links = [a for a in links if '/horse/' in a.get('href', '')]
        print(f"   é¦¬æƒ…å ±ãƒªãƒ³ã‚¯: {len(horse_links)}")
        if horse_links:
            print(f"   ã‚µãƒ³ãƒ—ãƒ«: {horse_links[0].get('href')}")
        
        # é¨æ‰‹ãƒªãƒ³ã‚¯
        jockey_links = [a for a in links if '/jockey/' in a.get('href', '')]
        print(f"   é¨æ‰‹æƒ…å ±ãƒªãƒ³ã‚¯: {len(jockey_links)}")
        
        # èª¿æ•™å¸«ãƒªãƒ³ã‚¯
        trainer_links = [a for a in links if '/trainer/' in a.get('href', '')]
        print(f"   èª¿æ•™å¸«æƒ…å ±ãƒªãƒ³ã‚¯: {len(trainer_links)}")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    analyze_shutuba_page()
